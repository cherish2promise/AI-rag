#  AI ë©´ì ‘ê´€ ì‹œìŠ¤í…œ (AI Interviewer Agent v2.0)

LangChain, OpenAI, LangGraph, ChromaDB ë“±ì„ í™œìš©í•˜ì—¬ ì´ë ¥ì„œ ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± â†’ ë‹µë³€ í‰ê°€ â†’ ìë™ ë³´ê³ ì„œ ìƒì„±ê¹Œì§€ ìë™í™”í•œ LLM ê¸°ë°˜ ì¸í„°ë·° ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

---

##  ì£¼ìš” ê¸°ëŠ¥

| ê¸°ëŠ¥ ëª¨ë“ˆ | ì„¤ëª… |
|----------|------|
| **ì´ë ¥ì„œ ë¶„ì„** (`analyze_resume`) | GPT-4oë¥¼ ì´ìš©í•´ ì´ë ¥ì„œë¥¼ ìš”ì•½í•˜ê³ , í•µì‹¬ í‚¤ì›Œë“œ/ê°•ì /ì•½ì  í‚¤ì›Œë“œ ì¶”ì¶œ |
| **ë©´ì ‘ ìŠ¤íƒ€ì¼ ì„¤ì •** (`set_style`) | êµ¬ì¡°í™”, ê¸°ìˆ , ìƒí™© ê¸°ë°˜ ë“± ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ ì¤‘ ë¬´ì‘ìœ„ 3ê°œ ì„ íƒ |
| **ì§ˆë¬¸ ìƒì„± ì „ëµ** (`generate_question_strategy`) | ìŠ¤íƒ€ì¼ë³„ ì§ˆë¬¸ ì „ëµ ì„¤ê³„ ë° ì˜ˆì‹œ ì§ˆë¬¸ êµ¬ì„± |
| **ì‹¬ì¸µ ì§ˆë¬¸ ìƒì„±** (`generate_question`) | ê¸°ì¡´ ì§ˆë¬¸, ë‹µë³€, í‰ê°€ ë‚´ìš©ì„ ë°˜ì˜í•œ ì‹¬í™” ì§ˆë¬¸ ìƒì„± ë° ë²¡í„° DB ì €ì¥ |
| **ë‹µë³€ í‰ê°€** (`evaluate_answer`) | ê´€ë ¨ì„±/êµ¬ì²´ì„± ê¸°ì¤€ìœ¼ë¡œ 5ì  ì²™ë„ í‰ê°€ ë° ì‚¬ìœ  ë¶„ì„ |
| **ìê¸° ë°˜ì„± í‰ê°€** (`reflection`) | ìµœê·¼ í‰ê°€ì˜ íƒ€ë‹¹ì„±ì„ ìê°€ íŒë‹¨í•˜ì—¬ íë¦„ ë¶„ê¸° (`ì •ìƒ`/`ì¬í‰ê°€ í•„ìš”`) |
| **íë¦„ ì œì–´** (`direction`) | ë‹¤ìŒ ì§„í–‰ ë‹¨ê³„ ê²°ì •: ì§ˆë¬¸, í‰ê°€, ë¦¬í”Œë ‰ì…˜, ìš”ì•½ ë“± |
| **ë©´ì ‘ ë³´ê³ ì„œ ìƒì„±** (`summarize_interview`) | ì „ì²´ ëŒ€í™”/í‰ê°€/ì „ëµì„ ë°”íƒ•ìœ¼ë¡œ ì„±í–¥ ë¶„ì„ + ì¢…í•© í”¼ë“œë°± ë¦¬í¬íŠ¸ ì¶œë ¥ |

---

##  ì‚¬ìš© ê¸°ìˆ 

- **LangChain + LangGraph**: ì¸í„°ë·° íë¦„ êµ¬ì„± ë° ìƒíƒœ ë¶„ê¸°
- **OpenAI GPT-4o-mini**: ì§ˆë¬¸/í‰ê°€/ìš”ì•½ ìƒì„±
- **ChromaDB**: ë²¡í„° ê¸°ë°˜ RAG êµ¬ì¡° êµ¬í˜„ (ë‹µë³€ íˆìŠ¤í† ë¦¬ ì €ì¥)
- **Gradio**: í”„ë¡ íŠ¸ì—”ë“œ UI (ì™¸ë¶€ ì—°ë™ ì‹œ)

---
## ì£¼ì˜ì‚¬í•­
- OpenAI API í‚¤ í•„ìš” (ChatOpenAI ì‚¬ìš© ì‹œ)
- Gradio UI ì—°ë™ì€ ì„ íƒì  (í•¨ìˆ˜ ê¸°ë°˜ CLI ì‹¤í–‰ ê°€ëŠ¥)
- ChromaDBëŠ” collection_name="interview_qa"ë¡œ ì„¤ì •ë¨


### ğŸ”¹ Step 3
```python
import pandas as pd
import numpy as np
import os
import ast
import fitz  # PyMuPDF
from docx import Document
import random
import openai
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

from typing import Annotated, Literal, Sequence, TypedDict

from langchain import hub
from langchain_core.messages import BaseMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langgraph.graph import StateGraph, START, END
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 4
```python
def load_api_keys(filepath="api_key.txt"):
    with open(filepath, "r") as f:
        for line in f:
            line = line.strip()
            if line and "=" in line:
                key, value = line.split("=", 1)
                os.environ[key.strip()] = value.strip()

path = '/content/drive/MyDrive/aivle/project_genai/'
# API í‚¤ ë¡œë“œ ë° í™˜ê²½ë³€ìˆ˜ ì„¤ì •
load_api_keys(path + 'api_key.txt')
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 5
```python
print(os.environ['OPENAI_API_KEY'][:30])
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 6
```python
def extract_text_from_file(file_path: str) -> str:
    ext = os.path.splitext(file_path)[1].lower()

    if ext == ".pdf":
        doc = fitz.open(file_path)
        text = "\n".join(page.get_text() for page in doc)
        doc.close()
        return text

    elif ext == ".docx":
        doc = Document(file_path)
        return "\n".join(p.text for p in doc.paragraphs if p.text.strip())

    else:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. PDF ë˜ëŠ” DOCXë§Œ í—ˆìš©ë©ë‹ˆë‹¤.")
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 7
```python
file_path = path + 'Resume_sample.pdf'
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 8
```python
resume_text = extract_text_from_file(file_path)
resume_text
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 9
```python
from typing import TypedDict, List, Dict

class InterviewState(TypedDict):
    # ê³ ì • ì •ë³´
    resume_text: str
    resume_summary: str
    resume_keywords: List[str]
    question_strategy: Dict[str, Dict]
    style : List[str]

    # ì¸í„°ë·° ë¡œê·¸
    current_question: str
    current_answer: str
    current_strategy: str
    conversation: List[Dict[str, str]]
    evaluation : List[Dict[str, int]]
    next_step : str

    #reflection
    reflection_status:str


    # ì˜ê²¬ í”¼ë“œë°±
    feedback : str
    tendency :str
    tot_opinion :str
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 10
```python
initial_state: InterviewState = {
    "resume_text": resume_text,
    "resume_summary": '',
    "resume_keywords": [],
    "question_strategy": {},
    "style" : [],
    "reflection_status" :'',

    "current_question": '',
    "current_answer": '',
    "current_strategy": '',
    "conversation": [],
    "evaluation": [],
    "next_step" : '',
    "feedback" : '',
    "tendency" : '',
    "tot_opinion":''
}

initial_state
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 11
```python
def analyze_resume(state: InterviewState) -> InterviewState:
    resume_text = state.get("resume_text", "")
    if not resume_text:
        raise ValueError("resume_textê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.")

    llm = ChatOpenAI(model="gpt-4o-mini")

    # ìš”ì•½ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    summary_prompt = ChatPromptTemplate.from_template(
        '''ë‹¹ì‹ ì€ ì´ë ¥ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸í„°ë·° ì§ˆë¬¸ì„ ì„¤ê³„í•˜ëŠ” AIì…ë‹ˆë‹¤.
        ë‹¤ìŒ ì´ë ¥ì„œ ë° ìê¸°ì†Œê°œì„œ ë‚´ìš©ì—ì„œ ì§ˆë¬¸ì„ ë½‘ê¸° ìœ„í•œ ì¤‘ìš”í•œ ë‚´ìš©ì„ 10ë¬¸ì¥ ì •ë„ë¡œ ìš”ì•½ì„ í•´ì¤˜(ìš”ì•½ì‹œ ** ê¸°í˜¸ëŠ” ì‚¬ìš©í•˜ì§€ ë§ê²ƒ):\n\n{resume_text}'''
    )
    formatted_summary_prompt = summary_prompt.format(resume_text=resume_text)
    summary_response = llm.invoke(formatted_summary_prompt)
    resume_summary = summary_response.content.strip()

    # í‚¤ì›Œë“œ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    keyword_prompt = ChatPromptTemplate.from_template(
        '''ë‹¹ì‹ ì€ ì´ë ¥ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¸í„°ë·° ì§ˆë¬¸ì„ ì„¤ê³„í•˜ëŠ” AIì…ë‹ˆë‹¤.
        ë‹¤ìŒ ì´ë ¥ì„œ ë° ìê¸°ì†Œê°œì„œë‚´ìš©ì—ì„œ ì§ˆë¬¸ì„ ë½‘ê¸° ìœ„í•œ ì¤‘ìš”í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ 5~10ê°œ ì¶”ì¶œí•´ì¤˜.
        ì¶”ê°€ë¡œ ì´ë ¥ì„œë¥¼ ë³´ê³  ê°•ì ê³¼ ì•½ì ì— ê´€ë ¨ëœ í‚¤ì›Œë“œë¥¼ ê°ê° 3ê°œì”© ì¶”ì¶œí•´ì¤˜.
        ë„ì¶œí•œ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì¤˜:\n\n{resume_text}'''
    )
    formatted_keyword_prompt = keyword_prompt.format(resume_text=resume_text)
    keyword_response = llm.invoke(formatted_keyword_prompt)

    parser = CommaSeparatedListOutputParser()
    resume_keywords = parser.parse(keyword_response.content)

    return {
        **state,
        "resume_summary": resume_summary,
        "resume_keywords": resume_keywords,
    }
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 12
```python
import random
styler=["êµ¬ì¡°í™” ë©´ì ‘","ë¹„êµ¬ì¡°í™” ë©´ì ‘", "ì—­ëŸ‰ ê¸°ë°˜ ë©´ì ‘","ìƒí™© ê¸°ë°˜ ë©´ì ‘","ê¸°ìˆ  ë©´ì ‘","ì¼€ì´ìŠ¤ ë©´ì ‘"]
style_set= random.sample(styler,3)
def set_style(state: InterviewState) -> InterviewState:
    # ì—¬ê¸°ì— ì½”ë“œë¥¼ ì™„ì„±í•©ë‹ˆë‹¤
    style=style_set
    return {
        **state,
        "style": style_set
    }
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 13
```python
i=0

def generate_question_strategy(state: InterviewState) -> InterviewState:
    global i
    # ì—¬ê¸°ì— ì½”ë“œë¥¼ ì™„ì„±í•©ë‹ˆë‹¤

    llm = ChatOpenAI(temperature=0.3, model="gpt-4o-mini")

    prompt = PromptTemplate(
      input_variables=["resume_summary", "resume_keywords","style"],
      template = """
        ì´ë ¥ì„œ ìš”ì•½: {resume_summary}
        í‚¤ì›Œë“œ: {resume_keywords}
        ì¤€ë¹„ëœ ì´ë ¥ì„œ ìš”ì•½ ,í•µì‹¬ í‚¤ì›Œë“œ ,ê°•ì  í‚¤ì›Œë“œ ,ì•½ì  í‚¤ì›Œë“œë¥¼ ì°¸ì¡°í•´ì„œ ë©´ì ‘ìì—ê²Œ ì§ˆë¬¸ì„ í•˜ë ¤ê³  í•´

        ë©´ì ‘ ìŠ¤íƒ€ì¼ì€ {style}ì— ë§ê²Œë” í•œê°œì˜ ì§ˆë¬¸ì„ ìƒì„±í•´ì¤˜
        """
    )

    chain = prompt | llm
    response = chain.invoke({
        "resume_summary": state["resume_summary"],
        "resume_keywords": ", ".join(state["resume_keywords"]),
        "style":state["style"][i]
    })

    strategy_dict = {}
    current_style = state["style"][i]

    strategy_dict[current_style] = {
        "ì˜ˆì‹œ ì§ˆë¬¸": [response.content.strip()]
    }

    i += 1

    # return ì½”ë“œëŠ” ì œê³µí•©ë‹ˆë‹¤.
    return {
        **state,
        "question_strategy": strategy_dict
    }
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 14
```python
def preProcessing_Interview(file_path: str) -> InterviewState:
    # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
    resume_text = extract_text_from_file(file_path)

    # 2. ì´ˆê¸° state êµ¬ì„±
    state: InterviewState = {
      "resume_text": resume_text,
      "resume_summary": '',
      "resume_keywords": [],
      "question_strategy": {},
      "style" : [],

      "current_question": '',
      "current_answer": '',
      "current_strategy": '',
      "conversation": [],
      "evaluation": [],
      "next_step" : '',
      "feedback" : '',
      "tendency" : '',
      "tot_opinion":'',
      "reflection_status":''
    }

    # 3. ì´ë ¥ì„œ ë¶„ì„
    state = analyze_resume(state)
    # style ì¶”ê°€
    state=set_style(state)
    # 4. ì§ˆë¬¸ ì „ëµ ìˆ˜ë¦½
    state = generate_question_strategy(state)

    # 5. ì²« ì§ˆë¬¸ ì¶”ì¶œ
    first_style = state["style"][0]
    strategy = state["question_strategy"].get(first_style, {})
    example_questions = strategy.get("ì˜ˆì‹œ ì§ˆë¬¸", [])

    selected_question = example_questions[0] if example_questions else "1ë¶„ ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”"

    return {
        **state,
        "current_question": selected_question,
        "current_strategy": "ì²« ì§ˆë¬¸"
    }
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 15
```python
state=preProcessing_Interview(file_path)
state
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 16
```python
def update_current_answer(state: InterviewState) -> InterviewState:

    return {
        **state,
        "current_answer": state["current_answer"].strip()
    }
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 17
```python
def evaluate_answer(state: InterviewState) -> InterviewState:
    from typing import List, Dict
    import re

    llm = ChatOpenAI(temperature=0.2, model="gpt-4o-mini")

    if state.get("reflection_status") == "ì •ìƒ":
        return state

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = ChatPromptTemplate.from_messages([
        ("system",
         """ë‹¹ì‹ ì€ ì¸ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë©´ì ‘ìì˜ ë‹µë³€ì„ ì•„ë˜ í•­ëª©ì— ë”°ë¼ í‰ê°€í•´ ì£¼ì„¸ìš”.

ê° í•­ëª©ë§ˆë‹¤ ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”:
1. ì ìˆ˜: 5~1ì  ì¤‘ í•˜ë‚˜ (ê°€ì¥ ë†’ì€ ì ìˆ˜ëŠ” 5ì  ê°€ì¥ ë‚®ì€ ì ìˆ˜ëŠ” 1ì )
2. ì‚¬ìœ : ê·¸ ì´ìœ ë¥¼ ì„œìˆ í˜•ìœ¼ë¡œ ì„¤ëª…

í˜•ì‹:
[ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±]
ì ìˆ˜: X
ì‚¬ìœ : ...

[ë‹µë³€ì˜ êµ¬ì²´ì„±]
ì ìˆ˜: X
ì‚¬ìœ : ...

ì§ˆë¬¸: {question}
ë‹µë³€: {answer}
"""),
        ("user", "ìœ„ ë‹µë³€ì„ í‰ê°€í•´ ì£¼ì„¸ìš”.")
    ])

    chain = prompt | llm
    result = chain.invoke({
        "question": state["current_question"],
        "answer": state["current_answer"],
    })

    content = result.content.strip()

    # ì‘ë‹µ íŒŒì‹±
    evaluation: List[Dict[str, str]] = []
    current_item = ""

    score = None
    reason = ""

    for line in content.splitlines():
        line = line.strip()
        if line.startswith("[ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±]"):
            current_item = "ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±"
        elif line.startswith("[ë‹µë³€ì˜ êµ¬ì²´ì„±]"):
            current_item = "ë‹µë³€ì˜ êµ¬ì²´ì„±"
        elif line.startswith("ì ìˆ˜:"):
            score_match = re.search(r"ì ìˆ˜:\s*(\d+)", line)
            if score_match:
                score = int(score_match.group(1))
        elif line.startswith("ì‚¬ìœ :"):
            reason = line.replace("ì‚¬ìœ :", "").strip()
            if current_item and score is not None:
                evaluation.append({
                    "í•­ëª©": current_item,
                    "ì ìˆ˜": score,
                    "ì‚¬ìœ ": reason
                })
                # ì´ˆê¸°í™”
                current_item = ""
                score = None
                reason = ""


    # ëŒ€í™” ê¸°ë¡
    conversation_entry = {
        "q": state["current_question"],
        "a": state["current_answer"]
    }

    return {
        **state,
        "evaluation": state["evaluation"] + evaluation,
        "conversation": state["conversation"] + [conversation_entry],
    }
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 18
```python
state=evaluate_answer(state)
state['evaluation']
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 19
```python
def reflection(state: InterviewState) -> InterviewState:
    llm = ChatOpenAI(temperature=0.2, model="gpt-4o-mini")

    # ìµœê·¼ í‰ê°€ í•­ëª©
    recent_eval = state["evaluation"][-2:] if len(state["evaluation"]) >= 2 else []
    eval_text = "\n".join([f"[{e['í•­ëª©']}] ì ìˆ˜: {e['ì ìˆ˜']}, ì‚¬ìœ : {e['ì‚¬ìœ ']}" for e in recent_eval])

    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ë©´ì ‘ìì…ë‹ˆë‹¤. ì•„ë˜ ì§ˆë¬¸ê³¼ ë‹µë³€, ê·¸ë¦¬ê³  í‰ê°€ ë‚´ìš©ì„ ë³´ê³  í•´ë‹¹ í‰ê°€ê°€ íƒ€ë‹¹í•˜ë‹¤ê³  ìƒê°í•˜ëŠ”ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

ê²°ê³¼ëŠ” ë°˜ë“œì‹œ "ì •ìƒ" ë˜ëŠ” "ì¬í‰ê°€ í•„ìš”" ì¤‘ í•˜ë‚˜ë¡œë§Œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}
ë‹µë³€: {answer}

í‰ê°€:
{evaluation}
"""),
        ("user", "ì´ í‰ê°€ëŠ” íƒ€ë‹¹í•˜ë‹¤ê³  ìƒê°í•˜ë‚˜ìš”?")
    ])

    chain = prompt | llm
    result = chain.invoke({
        "question": state["current_question"],
        "answer": state["current_answer"],
        "evaluation": eval_text
    })

    status = result.content.strip()
    if "ì¬í‰ê°€" in status:
        reflection_status = "ì¬í‰ê°€ í•„ìš”"
        # í‰ê°€ ì´ˆê¸°í™”
        updated_evaluation = state["evaluation"][:-2]  # ìµœê·¼ í‰ê°€ ì œê±°
    else:
        reflection_status = "ì •ìƒ"
        updated_evaluation = state["evaluation"]  # ìœ ì§€

    return {
        **state,
        "reflection_status": reflection_status,
        "evaluation": updated_evaluation
    }
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 20
```python
state = reflection(state)
state
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 21
```python
state=evaluate_answer(state)
state
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 22
```python
def direction(state: InterviewState) -> InterviewState:
    status = state.get("reflection_status", "").strip()

    if len(state["evaluation"])<3:
      if status == "ì •ìƒ":
          next_step = "generate_question"
      else:
          next_step = "reflection"
    else :
      next_step="summarize_interview"

    return {
        **state,
        "reflection_status" : '',
        "next_step": next_step
    }
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 23
```python
state['evaluation']
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 24
```python
state=direction(state)
state['next_step']
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 25
```python

```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 26
```python
state=generate_question(state)
state
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 27
```python
state["current_answer"]="ì•Œê³ ë¦¬ì¦˜ì„ ì˜ ì„ ì •í•˜ì§€ ëª»í•˜ê² ìŠµë‹ˆë‹¤."
state=update_current_answer(state)
state=evaluate_answer(state)
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 28
```python
state
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 29
```python
def summarize_interview(state: InterviewState) -> InterviewState:
    llm = ChatOpenAI(temperature=0.3, model="gpt-4o-mini")

    resume_text = state["resume_text"].strip()
    evaluation_text = "\n".join([
        f"{e['í•­ëª©']} - ì ìˆ˜: {e['ì ìˆ˜']}, ì‚¬ìœ : {e['ì‚¬ìœ ']}" for e in state["evaluation"]
    ])
    conversation_text = "\n".join([
        f"Q{i+1}: {qa['q']}\nA{i+1}: {qa['a']}" for i, qa in enumerate(state["conversation"])
    ])
    strategy_text = "\n".join([
        f"{k}: {', '.join(v['ì˜ˆì‹œ ì§ˆë¬¸'])}" for k, v in state["question_strategy"].items()
    ])

    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ì±„ìš© ë‹´ë‹¹ìë¡œ, ë‹¤ìŒ ë©´ì ‘ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì¸í„°ë·° ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.

[ì´ë ¥ì„œ]
{resume}

[ë©´ì ‘ ì§ˆë¬¸ ë° ë‹µë³€]
{conversation}

[í‰ê°€ ë‚´ìš©]
{evaluation}

[ì§ˆë¬¸ ì „ëµ]
{strategy}

--- ì‘ì„± ê¸°ì¤€ ---
1. ì„±í–¥ ë¶„ì„: ì§€ì›ìì˜ ëŒ€ë‹µì„ ê¸°ë°˜ìœ¼ë¡œ ì„±ê²©ì´ë‚˜ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìŠ¤íƒ€ì¼, ì—…ë¬´ íƒœë„ë¥¼ ë¶„ì„í•˜ì„¸ìš”.
2. ì „ì²´ ì˜ê²¬: ì§ˆë¬¸ ì „ëµë³„ ë‹µë³€ ìŠ¤íƒ€ì¼, ê°•ì ê³¼ ì•½ì ì„ ì¢…í•©ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.
""")
    ])

    chain = prompt | llm
    result = chain.invoke({
        "resume": resume_text,
        "conversation": conversation_text,
        "evaluation": evaluation_text,
        "strategy": strategy_text
    })

    content = result.content.strip()

    # ê°„ë‹¨í•œ ì¶”ì¶œ
    feedback, tendency, tot_opinion = "", "", ""
    lines = content.splitlines()
    current = None

    for line in content.splitlines():
      l = line.strip()
      if "ì„±í–¥ ë¶„ì„" in l:
        current = "tendency"
        continue
      elif "ì „ì²´ ì˜ê²¬" in l or "ê°•ì " in l:
        current = "tot_opinion"
        continue
      if current == "tendency":
        tendency += l + "\n"
      elif current == "tot_opinion":
        tot_opinion += l + "\n"

    # âœ… ìµœì¢… ë“±ê¸‰ ê³„ì‚°
    scores = [e["ì ìˆ˜"] for e in state["evaluation"] if isinstance(e["ì ìˆ˜"], (int, float))]
    avg_score = sum(scores) / len(scores) if scores else 0

    if avg_score >= 4.5:
        feedback = "ì¶”ì²œ"
    elif avg_score >= 3.5:
        feedback = "ì¡°ê±´ë¶€ ì¶”ì²œ"
    else:
        feedback = "ë¹„ì¶”ì²œ"

    # ë¦¬í¬íŠ¸ ì¶œë ¥
    report_lines = []
    report_lines.append("\nì¸í„°ë·° ì¢…ë£Œ ë¦¬í¬íŠ¸")
    report_lines.append("============================================================")
    report_lines.append("")

    report_lines.append("[1] ê¸°ë³¸ ì •ë³´")
    report_lines.append("ì§€ì›ì ì„±ëª… : í™ê¸¸ë™")  # ë‚˜ì¤‘ì— stateì—ì„œ ê°€ì ¸ì˜¤ë„ë¡ ê°œì„  ê°€ëŠ¥
    report_lines.append("ì§€ì› ì§ë¬´ / ë¶€ì„œ : AI ê°œë°œíŒ€")
    report_lines.append("")

    for i, qa in enumerate(state["conversation"]):
        report_lines.append(f"ì§ˆë¬¸ {i+1}: {qa['q']}")
        report_lines.append(f"ë‹µë³€ {i+1}: {qa['a']}")
        report_lines.append("")

    report_lines.append("------------------------------------------------------------")
    report_lines.append("[2] ì¢…í•© í‰ê°€")
    report_lines.append(f"ì¶”ì²œ ì—¬ë¶€: {feedback}")
    report_lines.append("")

    report_lines.append("[3] ì¢…í•© ì˜ê²¬ ë° í”¼ë“œë°±")
    report_lines.append(f"ì„±í–¥ ë¶„ì„: {tendency}")
    report_lines.append("")
    report_lines.append("ì „ì²´ ì˜ê²¬:")
    report_lines.append(tot_opinion)
    report_lines.append("============================================================")

    print("\n".join(report_lines))

    return {
        **state,
        "feedback": feedback.strip(),
        "tendency": tendency.strip(),
        "tot_opinion": tot_opinion.strip()
    }
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 30
```python
state = summarize_interview(state)
state
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 31
```python
# 6) Agent --------------------
# ë¶„ê¸° íŒë‹¨ í•¨ìˆ˜
# â”€â”€â”€ graph ì„¤ì • ë¶€ë¶„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from langgraph.graph import StateGraph, END

graph = StateGraph(InterviewState)

# update_answer ë…¸ë“œ ë“±ë¡/ì—°ê²°ì„ ëª¨ë‘ ë¹¼ê³ ,
# ì§„ì…ì ì„ evaluate_answer ë¡œ ì§€ì •
graph.set_entry_point("evaluate_answer")

graph.add_node("evaluate_answer", evaluate_answer)
graph.add_node("reflection", reflection)
graph.add_node("direction", direction)
graph.add_node("generate_question", generate_question)
graph.add_node("summarize_interview", summarize_interview)

# evaluate_answer ì´í›„ë¡œëŠ” ê¸°ì¡´ ë¶„ê¸° ë¡œì§ ê·¸ëŒ€ë¡œ
graph.add_edge("evaluate_answer", "direction")
graph.add_conditional_edges("direction", lambda s: s["next_step"], {
    "reflection": "reflection",
    "generate_question": "generate_question",
    "summarize_interview": "summarize_interview",
})
graph.add_edge("reflection", "evaluate_answer")
graph.add_edge("generate_question", "evaluate_answer")  # ì´ ë¶€ë¶„ ê°„ë‹¨íˆ ë„˜ê²¨ë„ ë©ë‹ˆë‹¤.
graph.add_edge("summarize_interview", END)


# ìµœì¢… ë¹Œë“œ
interview_app = graph.compile()
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 32
```python
# íŒŒì¼ ì…ë ¥
file_path = path + 'Resume_sample.pdf'
i=0
state_s = preProcessing_Interview(file_path)
state_s
```
**ì„¤ëª…:**
- 

### ğŸ”¹ Step 33
```python
# ì‚¬ìš©ì ì‘ë‹µ ë£¨í”„
state_s = interview_app.invoke(state_s)
```
**ì„¤ëª…:**
- 


